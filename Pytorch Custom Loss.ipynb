{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss\n",
    "\n",
    "- use nn.Module\n",
    "\n",
    "- use functions\n",
    "\n",
    "- use direct alrithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using nn.Module\n",
    "https://spandan-madan.github.io/A-Collection-of-important-tasks-in-pytorch/\n",
    "\n",
    "Here I show a custom loss called Regress_Loss\n",
    "\n",
    "- it takes as input 2 kinds of input x and y. \n",
    "- it reshapes x to be similar to y\n",
    "- it finally returns the loss by calculating L2 difference between reshaped x and y. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9837, 0.9752, 0.9653]], device='cuda:0', requires_grad=True)\n",
      "tensor([[0.9978, 0.9967, 0.9954]], device='cuda:0', requires_grad=True)\n",
      "tensor([[0.9997, 0.9996, 0.9994]], device='cuda:0', requires_grad=True)\n",
      "tensor([[1.0000, 0.9999, 0.9999]], device='cuda:0', requires_grad=True)\n",
      "tensor([[1.0000, 1.0000, 1.0000]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## define loss\n",
    "class Regress_Loss(torch.nn.Module):\n",
    "    ## initialization\n",
    "    def __init__(self):\n",
    "        super(Regress_Loss,self).__init__() ## initialization         \n",
    "    ## forward \n",
    "    def forward(self, Output, Target):\n",
    "        assert(Output.shape == Target.shape)\n",
    "        loss = (Output - Target).pow(2).sum()\n",
    "        return loss\n",
    "\n",
    "## initialize tensors\n",
    "X = torch.ones((1, 3),  device = device, requires_grad = False)\n",
    "Y = torch.randn_like(X, device = device, requires_grad = True)\n",
    "\n",
    "## initialize loss\n",
    "criterion = Regress_Loss()\n",
    "lr = 0.01\n",
    "\n",
    "## run optimization\n",
    "for epoch in range(500):\n",
    "    loss = criterion.forward(X, Y)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        Y -= Y.grad * lr\n",
    "        Y.grad.zero_()\n",
    "        if epoch % 100 == 99:\n",
    "            print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss with function\n",
    "\n",
    "Note that backpropagation is handled by Variables, and not by nn.Module.\n",
    "\n",
    "You donâ€™t need to write a nn.Module for that, a simple function is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4363, -1.4599,  0.0363]], device='cuda:0', requires_grad=True)\n",
      "tensor([[0.9252, 0.6738, 0.8722]], device='cuda:0', requires_grad=True)\n",
      "tensor([[0.9901, 0.9567, 0.9831]], device='cuda:0', requires_grad=True)\n",
      "tensor([[0.9987, 0.9943, 0.9978]], device='cuda:0', requires_grad=True)\n",
      "tensor([[0.9998, 0.9992, 0.9997]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "lr = 0.01\n",
    "def power(t, n): return t ** n\n",
    "X = torch.ones((1, 3),  device = device, requires_grad = False)\n",
    "Y = torch.randn_like(X, device = device, requires_grad = True)\n",
    "\n",
    "for epoch in range(500):\n",
    "    ## calculate loss\n",
    "    loss = power(Y - X,2.0).sum() ## neat work here, function in one line\n",
    "    loss.backward()    \n",
    "    ## update loss\n",
    "    with torch.no_grad():\n",
    "        Y -= Y.grad * lr\n",
    "        Y.grad.zero_() ## like optimizer.zero_grad()\n",
    "        if epoch % 100 == 0:\n",
    "            print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "input_dims = 128\n",
    "hidden_size= 32\n",
    "output_dims= 8\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(input_dims,  batch_size, device=device)\n",
    "y = torch.randn(output_dims, batch_size, device=device)\n",
    "w1 = torch.randn(hidden_size, input_dims,  device=device, requires_grad=True)\n",
    "w2 = torch.randn(output_dims, hidden_size, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 253814.78125\n",
      "50 10.663934707641602\n",
      "100 0.8452450633049011\n",
      "150 0.0856180340051651\n",
      "200 0.01071980595588684\n",
      "250 0.0021665175445377827\n",
      "300 0.0007943820091895759\n",
      "350 0.00039197015576064587\n",
      "400 0.0002094614173984155\n",
      "450 0.00011387151607777923\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "\n",
    "    y_pred = w2.mm(w1.mm(x).clamp(min=0))\n",
    "    loss = (y_pred - 1).pow(2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        if epoch % 50 == 0:\n",
    "            print(epoch, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8627,  0.2651, -0.9993, -9.2004, -9.4639],\n",
       "        [-0.5937, -1.5699, -0.2681, -1.3841, -0.1898],\n",
       "        [ 0.3971, -0.9961, -2.0465, -0.4930,  0.4887],\n",
       "        [-0.5223,  0.0751, -8.9803, -1.3236, -0.7704],\n",
       "        [-9.3694, -0.5970,  0.0582,  0.3238, -0.0877]])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = torch.randn((5,5))\n",
    "T[T>0.5] = -10 + T[T>0.5]\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4673,  0.4062, -0.7304,  0.0000, -0.6690],\n",
       "        [-0.4327,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.7553, -0.0244, -0.6737, -0.2495, -1.5302],\n",
       "        [-0.6705,  0.0000,  0.2208,  0.3384,  0.2874],\n",
       "        [ 0.0000,  0.2077,  0.0000,  0.0000, -0.5580]])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
